# ğŸ” ML Daily Byte â€“ Day 7: Data Leakage in Machine Learning

Data leakage is one of the **most dangerous and sneaky mistakes** in machine learning.  
It leads to **inflated performance metrics** and models that **fail in the real world**.

---

## ğŸ§  What is Data Leakage?

**Data Leakage** occurs when the training data contains information that **would not be available at prediction time**.  
This causes the model to **cheat unknowingly**, resulting in misleadingly high accuracy.

---

## ğŸš¨ Why Itâ€™s a Problem

- Makes the model look "perfect" on test data
- Fails badly in real-world scenarios
- Reinforces bias or past decisions without true learning

---

## ğŸ¥ Real-World Analogy: Health Model

Imagine building a model to **predict heart attacks**, but you include a column `treatment_given` â€” which is only recorded **after the diagnosis**.  
The model just learns:  
> "If treatment was given â†’ must be a heart attack!"  
Not smart â€” thatâ€™s **leakage**.

---

## ğŸ“ Social Analogy: Education Loan Model

Suppose youâ€™re building a model to predict **education loan approval** for students.

If you include the feature `loan_approved` (the actual outcome) in your training data:

- Model says â€œyesâ€ only because the past system did  
- It doesn't learn *why* students get approved  
- May unfairly deny deserving new applicants

ğŸ“Œ **Impact**: Affects low-income students. Ethical ML starts with clean data!

---

## ğŸ” Types of Data Leakage

| Type                     | Description                                                                 |
|--------------------------|-----------------------------------------------------------------------------|
| **Target Leakage**       | Features contain info that wonâ€™t be known at prediction time                |
| **Train-Test Contamination** | Test data leaks into training pipeline during preprocessing or scaling     |
| **Temporal Leakage**     | Model sees future data when predicting earlier events                       |

---

## ğŸ‘¨â€ğŸ’» Code Example: Leaky vs Clean Model

```python
# Simulated dataset
data = {
    'age': [50, 60, 45, 35, 70, 30],
    'bp': [120, 140, 130, 110, 150, 115],
    'cholesterol': [200, 240, 180, 170, 250, 190],
    'diagnosis': [1, 1, 0, 0, 1, 0],
    'treatment_given': [1, 1, 0, 0, 1, 0]  # ğŸš¨ Future info!
}
df = pd.DataFrame(data)

# Leaky features
X_leak = df[['age', 'bp', 'cholesterol', 'treatment_given']]
y = df['diagnosis']

# Clean features
X_clean = df[['age', 'bp', 'cholesterol']]

**ğŸ§  Key Takeaways**
Donâ€™t include features unavailable at real-world prediction time

Always split train/test before preprocessing

Watch out for timestamp and target leakage

Ask: â€œWould I know this at prediction time?â€
**ğŸ”— Resources**
Kaggle: Avoiding Data Leakage

Scikit-learn Guide on Data Preprocessing

Blog: Data Leakage in ML

#DataLeakage #EthicalAI #MLDailyByte #EducationAI #ModelValidation
#TrainTestSplit #MachineLearning #Python #DataScience #Generalization
