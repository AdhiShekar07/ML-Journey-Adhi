# ğŸ“˜ ML Daily Byte â€“ Day 2: Data Splitting & Spam Detection

Welcome to **Day 2** of my Machine Learning learning series.  
Today, I explored one of the most important concepts in ML development: **splitting data** correctly â€” and applied that knowledge to a practical **Spam Email Classifier** project using the Naive Bayes algorithm.

---

## ğŸ” Why Split Data?

Before training any ML model, we need to divide our dataset so that we can evaluate how well our model performs on **unseen data**. This helps prevent **overfitting** and improves model **generalization**.

### âœ¨ Real-Life Analogy:
Think of preparing for an exam:
- ğŸ“š **Training Set** â€“ notes and materials you study
- ğŸ“ **Validation Set** â€“ practice tests to adjust your learning
- ğŸ“ **Test Set** â€“ your final exam that tests your understanding

---

## ğŸ”„ Common Data Split Ratios

| Set          | Purpose                      | Typical Split |
|--------------|------------------------------|----------------|
| Training     | Learn patterns from data      | 60â€“80%         |
| Validation   | Tune model performance        | 10â€“20%         |
| Testing      | Final unbiased evaluation     | 10â€“20%         |

---

## ğŸŒ¼ Example 1: Iris Dataset Split

A classic dataset for beginners in ML. Hereâ€™s a basic example of how to split it:

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split into 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data size:", len(X_train))
print("Testing data size:", len(X_test))
