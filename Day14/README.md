# ğŸ“˜ Day 14 â€“ Activation Functions (ReLU, Sigmoid, Tanh, GELU)

---

## ğŸ§  Concept Overview

Activation functions are the **decision-makers in a neural network**.  
They decide whether a neuron should activate (pass signal) or not.

Without them, your model would just be a linear calculator.  
With them, your model can learn **complex patterns**, just like a human brain learns from different situations.

---

## ğŸ© Real-Life Analogy â€“ Smart Bakery Decision System

Imagine a smart bakery ğŸ© that only bakes when thereâ€™s an order.

- No order? Do nothing.  
- Small order? Bake slowly.  
- Big order? Bake full speed!

Each **activation function** is like a rule the bakery follows for deciding **how much to bake**:

| Activation | Behavior | Bakery Logic |
|------------|----------|--------------|
| **ReLU**   | Only positive values pass | â€œIf order > 0, start baking!â€ |
| **Sigmoid**| Smooth yes/no decision between 0 and 1 | â€œMaybe bake a little...â€ |
| **Tanh**   | Balanced output from -1 to +1 | â€œBake less or more based on needâ€ |
| **GELU**   | Smart + slightly random activation | â€œCreative baking decisionsâ€ |

---

## ğŸ¥ Real-World Use Case: Medical Diagnosis AI

Youâ€™re building a neural network that predicts if a person has a disease based on health data.

- **Sigmoid** in the output layer tells:  
  â¤ â€œThereâ€™s an 82% chance the person has the disease.â€

- **ReLU** in hidden layers passes only meaningful symptoms.

- **GELU** is used in models like ChatGPT & BERT for stable learning.

---

## ğŸ’» Code Example: Visualizing Activation Functions

This code shows how ReLU, Sigmoid, and Tanh behave on input data using graphs.

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Sample input values
X = np.linspace(-10, 10, 100)

# Apply activation functions
relu = tf.nn.relu(X)
sigmoid = tf.nn.sigmoid(X)
tanh = tf.nn.tanh(X)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(X, relu, label='ReLU', color='red')
plt.plot(X, sigmoid, label='Sigmoid', color='green')
plt.plot(X, tanh, label='Tanh', color='blue')

plt.title("Activation Functions: ReLU vs Sigmoid vs Tanh")
plt.xlabel("Input Value")
plt.ylabel("Activated Output")
plt.grid(True)
plt.legend()
plt.show()
