# 🧠 UN Interpreter Transformer – Day 27

This project demonstrates a **basic Transformer architecture** using a real-world analogy of a **UN interpreter**. It highlights how **Self-Attention** and **Encoder-Decoder** mechanisms work in machine translation tasks.

## 📌 Concept
Imagine a **UN conference**:
- **Speakers** from different countries talk in various languages.
- A **skilled interpreter** listens, understands context, and translates into a common target language.

This simulates the **Transformer model**:
- The **Encoder** processes source sentences (speakers).
- The **Decoder** generates translated output (interpreter's words).
- **Self-Attention** ensures each word considers its context while encoding/decoding.

## 🛠️ Tech Stack
- Python 🐍
- PyTorch 🔥

## 📂 Files
- `UNInterpreterTransformer.py`: Core implementation of a simplified Transformer block with self-attention and encoder-decoder structure.

## 🚀 How to Run
```bash
pip install torch
python UNInterpreterTransformer.py
