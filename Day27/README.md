# ğŸ§  UN Interpreter Transformer â€“ Day 27

This project demonstrates a **basic Transformer architecture** using a real-world analogy of a **UN interpreter**. It highlights how **Self-Attention** and **Encoder-Decoder** mechanisms work in machine translation tasks.

## ğŸ“Œ Concept
Imagine a **UN conference**:
- **Speakers** from different countries talk in various languages.
- A **skilled interpreter** listens, understands context, and translates into a common target language.

This simulates the **Transformer model**:
- The **Encoder** processes source sentences (speakers).
- The **Decoder** generates translated output (interpreter's words).
- **Self-Attention** ensures each word considers its context while encoding/decoding.

## ğŸ› ï¸ Tech Stack
- Python ğŸ
- PyTorch ğŸ”¥

## ğŸ“‚ Files
- `UNInterpreterTransformer.py`: Core implementation of a simplified Transformer block with self-attention and encoder-decoder structure.

## ğŸš€ How to Run
```bash
pip install torch
python UNInterpreterTransformer.py
